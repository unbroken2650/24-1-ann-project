{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import callbacks\n",
    "import time\n",
    "from models.lenet import LeNetModel\n",
    "from models.resnet50 import ResNetModel\n",
    "from models.residual import ResModel\n",
    "from models.simple_cnn import CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../../../../mnt/sda/suhohan/emnist/emnist-byclass-train.csv'\n",
    "test_file_path = '../../../../mnt/sda/suhohan/emnist/emnist-byclass-test.csv'\n",
    "\n",
    "chunk_size = 10000\n",
    "train_data_iter = pd.read_csv(train_file_path, chunksize=chunk_size)\n",
    "train_data = pd.concat([chunk for chunk in tqdm(train_data_iter, desc='Loading training data')])\n",
    "test_data_iter = pd.read_csv(test_file_path, chunksize=chunk_size)\n",
    "test_data = pd.concat([chunk for chunk in tqdm(test_data_iter, desc='Loading test data')])\n",
    "\n",
    "# Data dimensions and sizes\n",
    "num_train_samples = train_data.shape[0]\n",
    "num_test_samples = test_data.shape[0]\n",
    "\n",
    "# Prepare data\n",
    "x_train = train_data.iloc[:, 1:].to_numpy().reshape((num_train_samples, 28, 28, 1))\n",
    "x_test = test_data.iloc[:, 1:].to_numpy().reshape((num_test_samples, 28, 28, 1))\n",
    "y_train = tf.keras.utils.to_categorical(train_data.iloc[:, 0], 62)  # 62 classes for EMNIST ByClass\n",
    "y_test = tf.keras.utils.to_categorical(test_data.iloc[:, 0], 62)\n",
    "\n",
    "# Integer labels for sparse categorical crossentropy\n",
    "y_train_int = train_data.iloc[:, 0].to_numpy()\n",
    "y_test_int = test_data.iloc[:, 0].to_numpy()\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "_, _, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "x_train, x_valid, y_train_int, y_valid_int = train_test_split(x_train, y_train_int, test_size=0.1, random_state=42)\n",
    "\n",
    "# Prepare data for ResNet\n",
    "x_train_resized = tf.image.resize(x_train, [32, 32])\n",
    "x_valid_resized = tf.image.resize(x_valid, [32, 32])\n",
    "x_test_resized = tf.image.resize(x_test, [32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_lenet = f\"./checkpoints/checkpoints_lenet/weights.{int(time.time())}.hdf5\"\n",
    "checkpoint_path_resnet = f\"./checkpoints/checkpoints_resnet/weights.{int(time.time())}.hdf5\"\n",
    "checkpoint_path_ours = f\"./checkpoints/checkpoints_ours/weights.{int(time.time())}.hdf5\"\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.000001)\n",
    "checkpoint_lenet = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path_lenet, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "checkpoint_resnet = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path_resnet, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "checkpoint_ours = callbacks.ModelCheckpoint(filepath=checkpoint_path_ours,\n",
    "                                            monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "lenet_model = LeNetModel()\n",
    "resnet_model = ResNetModel()\n",
    "our_model_1 = CNNModel()\n",
    "our_model_2 = ResModel()\n",
    "\n",
    "# Compile models\n",
    "lenet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "our_model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "our_model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "training_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history.append(lenet_model.train(x_train, y_train_int, validation_data=(x_valid, y_valid_int),\n",
    "               epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[reduce_lr, checkpoint_lenet]))\n",
    "end_time = time.time()\n",
    "training_time.append(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history.append(resnet_model.train(x_train_resized, y_train_int, validation_data=(x_valid_resized,\n",
    "               y_valid_int), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[reduce_lr, checkpoint_resnet]))\n",
    "end_time = time.time()\n",
    "training_time.append(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history.append(our_model_1.train(x_train, y_train_int, validation_data=(x_valid, y_valid_int),\n",
    "               epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[reduce_lr, checkpoint_ours]))\n",
    "end_time = time.time()\n",
    "training_time.append(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history.append(our_model_2.train(x_train, y_train_int, validation_data=(x_valid, y_valid_int),\n",
    "               epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[reduce_lr, checkpoint_ours]))\n",
    "end_time = time.time()\n",
    "training_time.append(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lenet, acc_lenet = lenet_model.evaluate(x_test, y_test_int)\n",
    "loss_resnet, acc_resnet = resnet_model.evaluate(x_test_resized, y_test_int)\n",
    "loss_our1, acc_our1 = our_model_1.evaluate(x_test, y_test_int)\n",
    "loss_our2, acc_our2 = our_model_2.evaluate(x_test, y_test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "_time = datetime.strftime(datetime.today(), '%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Model\": [\"LeNet-5\", \"ResNet-50\", \"Our Model(CNN)\", \"Our Model(Residual)\"],\n",
    "    \"Loss\": [loss_lenet, loss_resnet, loss_our1, loss_our2],\n",
    "    \"Accuracy\": [acc_lenet, acc_resnet, acc_our1, acc_our2],\n",
    "    \"Training Time\": training_time\n",
    "}\n",
    "result_path = './results'\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{result_path}/result_{_time}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"blue\", \"green\", \"purple\", \"gold\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "for idx, hist in enumerate(history):\n",
    "    plt.plot(hist.history['loss'], label=f'{results[\"Model\"][idx]}', color=colors[idx])\n",
    "    plt.title(f'Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlim([0, EPOCHS])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "plt.subplot(122)\n",
    "for idx, hist in enumerate(history):\n",
    "    plt.plot(hist.history['accuracy'], label=f'{results[\"Model\"][idx]}', color=colors[idx])\n",
    "    plt.title(f'Train Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim([0, EPOCHS])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{result_path}/result_{_time}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_time.append(\"\")\n",
    "results = {\n",
    "    \"Model\": [\"LeNet-5\", \"ResNet-50\", \"Our Model\", \"Wavemix-256[6]\"],\n",
    "    \"Loss\": [loss_lenet, loss_resnet, loss_our1, loss_our2, 0],\n",
    "    \"Accuracy\": [acc_lenet, acc_resnet, acc_our1, acc_our2, 0.8842],\n",
    "    \"Training Time\": training_time\n",
    "}\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "result_path = './results'\n",
    "metrics = [\"Loss\", \"Accuracy\", \"Training Time\"]\n",
    "limits = [[0, 0.8], [0.5, 1.0], [0, 3500]]\n",
    "for metric, y_limit in zip(metrics, limits):\n",
    "    plt.figure(figsize=(5, 6))\n",
    "    sns.barplot(x=\"Model\", y=metric, data=results_df, palette=\"viridis\")\n",
    "    plt.title(f'{metric} Comparison')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylim(y_limit)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_path}/{metric.lower().replace(\" \", \"_\")}_comparison_{_time}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model_2.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
