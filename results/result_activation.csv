Activation Function,Loss,Accuracy,Training Time
relu,0.30963823199272156,0.8831003308296204,644.1749358177185
leaky_relu,0.3094892203807831,0.8816303014755249,1165.668054819107
elu,0.3416938781738281,0.8718643188476562,1352.1042454242706
selu,0.3809724748134613,0.8625023365020752,1344.599228143692
sigmoid,0.44148069620132446,0.8429703712463379,1364.8263955116272
tanh,0.3695305287837982,0.8659582734107971,1330.5614759922028
softmax,3.250260829925537,0.5826584696769714,1453.9030430316925
softplus,0.37335842847824097,0.8625797629356384,1365.1570415496826
softsign,0.40921837091445923,0.8549543619155884,1338.1028094291687
swish,0.3639970123767853,0.865674614906311,1553.0467464923859
gelu,0.36376693844795227,0.8660270571708679,1719.5532610416412
