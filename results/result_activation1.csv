Activation Function,Loss,Accuracy,Training Time
relu,0.3279823660850525,0.8754921555519104,246.59434628486633
leaky_relu,0.3335038721561432,0.8740736842155457,243.27766799926758
elu,0.3296789526939392,0.8753889799118042,243.33447074890137
selu,0.3339484632015228,0.8735578656196594,242.69469547271729
sigmoid,0.35647985339164734,0.8677893877029419,242.40995001792908
tanh,0.3444490432739258,0.8723199367523193,242.10587167739868
softmax,0.5575599670410156,0.8358951807022095,269.276424407959
softplus,0.3398593068122864,0.872543454170227,247.77890348434448
softsign,0.3499597907066345,0.8692680597305298,243.3401939868927
swish,0.3177199065685272,0.878999650478363,286.3518543243408
gelu,0.31465429067611694,0.8806674480438232,322.21487832069397
