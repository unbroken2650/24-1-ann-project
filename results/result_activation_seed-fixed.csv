Activation Function,Loss,Accuracy,Training Time
relu,0.33277642726898193,0.8719760775566101,242.81793594360352
leaky_relu,0.3321162462234497,0.8740478754043579,244.90939331054688
elu,0.3289371430873871,0.8746668696403503,246.58716297149658
selu,0.3384242653846741,0.872302770614624,245.21213388442993
sigmoid,0.40017345547676086,0.8519798517227173,242.7621500492096
tanh,0.3395417332649231,0.8719846606254578,244.2050907611847
softmax,0.5600265264511108,0.8340640664100647,273.5112946033478
softplus,0.34379124641418457,0.8689413666725159,248.78319883346558
softsign,0.34761813282966614,0.8701621294021606,243.27491188049316
swish,0.31635957956314087,0.8791028261184692,286.791228055954
gelu,0.31628352403640747,0.8784666657447815,321.9677267074585
