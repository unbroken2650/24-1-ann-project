Activation Function,Loss,Accuracy,Training Time
relu,0.3243207037448883,0.8768762350082397,244.23910784721375
leaky_relu,0.33519837260246277,0.8738330006599426,247.33267498016357
elu,0.3267273008823395,0.8768676519393921,245.5177149772644
selu,0.3454720079898834,0.8691391348838806,245.51192998886108
sigmoid,0.36193108558654785,0.8647547364234924,245.26052331924438
tanh,0.3411422669887543,0.8725262880325317,245.66175532341003
softmax,0.40564027428627014,0.8614449501037598,270.25555324554443
softplus,0.3341537117958069,0.8731968402862549,251.38134145736694
softsign,0.34551066160202026,0.8718041181564331,247.071631193161
swish,0.32139068841934204,0.8784322738647461,286.96050214767456
gelu,0.31718888878822327,0.879910945892334,327.7681267261505
